{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-28T06:09:10.714124Z","iopub.execute_input":"2022-07-28T06:09:10.714579Z","iopub.status.idle":"2022-07-28T06:09:10.751267Z","shell.execute_reply.started":"2022-07-28T06:09:10.714499Z","shell.execute_reply":"2022-07-28T06:09:10.750407Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**Notebook should contain :**\n1. EDA, Feature selection, preprocessing\n2. Model performance analysis in terms of validation and risks involved\n3. model predictions for test dataset\n4. write the dependencies, libraries in seperate python file\n5. Readme file - approach to solve prob, thought process\n\n**Task steps:**\n1. Split the train set into train and validation in 4:1 ratio\n2. Explain model selection. Apply classification model \n3. Evaluate model accuracy\n\n","metadata":{}},{"cell_type":"markdown","source":"## Approach:\n\nNotebook 1: \n\n1. Load and split the data \n\n2. Standardize and Pipeline tree-based algorithms to handle data imbalances\n\n3. Obtain algorithms with highest accuracy\n\n4. Tune the algorithm and find best parameters\n\nNotebook 2:\n\n1. Explore the dataset \n    - Check data relations\n    - Data correlations\n    - Missing values\n    - Outliers\n    - Different data types\n    - Fix data distribution skewness, kurtosis\n    - Fix outliers\n    - Scale, undersample the data\n    \n2. Split the cleaned data into 3 sets ( Test, Train, validation set)\n\n3. Utilise 5 fold validation and compare accuracy / recall / roc-auc scores for train, test and validation sets \n\n4. Take the parameters and model from notebook1. Integrate data with this model.\n\n5. Plot confusion matrix, roc-auc curves and expected-actual prediction\n","metadata":{}},{"cell_type":"markdown","source":"## Load the data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/aryaai-dataset/Arya_DataScientist_Assignment/training_set.csv')\ntest_df = pd.read_csv('../input/aryaai-dataset/Arya_DataScientist_Assignment/test_set.csv')","metadata":{"execution":{"iopub.status.busy":"2022-07-28T06:09:10.753263Z","iopub.execute_input":"2022-07-28T06:09:10.754018Z","iopub.status.idle":"2022-07-28T06:09:10.818593Z","shell.execute_reply.started":"2022-07-28T06:09:10.753981Z","shell.execute_reply":"2022-07-28T06:09:10.817666Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2022-07-28T06:09:10.821123Z","iopub.execute_input":"2022-07-28T06:09:10.821801Z","iopub.status.idle":"2022-07-28T06:09:10.848726Z","shell.execute_reply.started":"2022-07-28T06:09:10.821764Z","shell.execute_reply":"2022-07-28T06:09:10.847760Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# import seaborn as sns\n\n# sns.pairplot(train_df,hue='Y',size=3)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T06:09:10.851306Z","iopub.execute_input":"2022-07-28T06:09:10.851657Z","iopub.status.idle":"2022-07-28T06:09:10.855478Z","shell.execute_reply.started":"2022-07-28T06:09:10.851623Z","shell.execute_reply":"2022-07-28T06:09:10.854412Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"x = train_df.drop('Y',axis=1)\ny = train_df['Y']","metadata":{"execution":{"iopub.status.busy":"2022-07-28T06:09:10.857335Z","iopub.execute_input":"2022-07-28T06:09:10.857690Z","iopub.status.idle":"2022-07-28T06:09:10.869552Z","shell.execute_reply.started":"2022-07-28T06:09:10.857654Z","shell.execute_reply":"2022-07-28T06:09:10.868551Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(x, y,train_size=0.33)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T06:09:10.871147Z","iopub.execute_input":"2022-07-28T06:09:10.871486Z","iopub.status.idle":"2022-07-28T06:09:11.917349Z","shell.execute_reply.started":"2022-07-28T06:09:10.871438Z","shell.execute_reply":"2022-07-28T06:09:11.916070Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(xtrain.shape,'\\n',xtest.shape,'\\n',ytrain.shape,'\\n',ytest.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T06:09:11.923327Z","iopub.execute_input":"2022-07-28T06:09:11.926137Z","iopub.status.idle":"2022-07-28T06:09:11.936264Z","shell.execute_reply.started":"2022-07-28T06:09:11.926097Z","shell.execute_reply":"2022-07-28T06:09:11.935077Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Let's try out the logistic regression algorithm with a pipeline to see how well the numerical data fits to a curve.","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\npipeline = Pipeline([('normalizer', StandardScaler()), #Step1 - normalize data\n    ('clf', LogisticRegression())])\nscores = cross_validate(pipeline, xtrain, ytrain)\n# print(scores)\nprint(scores['test_score'].mean())","metadata":{"execution":{"iopub.status.busy":"2022-07-28T06:09:11.940527Z","iopub.execute_input":"2022-07-28T06:09:11.942233Z","iopub.status.idle":"2022-07-28T06:09:12.513924Z","shell.execute_reply.started":"2022-07-28T06:09:11.942196Z","shell.execute_reply":"2022-07-28T06:09:12.512583Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"The Avg test accuracy of logistic regression is 90.31%.\n\nCurious to checkout the score of other classification algorithms.","metadata":{}},{"cell_type":"markdown","source":"## Pipelining classification algorithms \n\nWe want to find this algorithm and improve its accuracy by cross-validating. \n\n> - Dtree\n> - GBDT\n> - SVM\n> - LogReg\n> - KNN","metadata":{}},{"cell_type":"code","source":"\ncl = []\ncl.append(DecisionTreeClassifier())\ncl.append(RandomForestClassifier())\ncl.append(GradientBoostingClassifier())\ncl.append(LogisticRegression())\ncl.append(SVC())\n\npipeline = Pipeline([('normalizer', StandardScaler()), #Step1 - normalize data\n    ('clf', LogisticRegression())])\npipeline.steps\nfor c in cl:\n    pipeline.set_params(clf = c)\n    scores = cross_validate(pipeline, xtrain,ytrain)\n    print('_'*25)\n    print(str(c))\n    print('_'*25)\n    print(scores['test_score'].mean())\n#     for k, v in scores.items():\n#         print(k,'mean',v.mean())\n#         print(k,'std',v.std())\n#     print('_'*25)\n    \n    \n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T06:09:12.515333Z","iopub.execute_input":"2022-07-28T06:09:12.515675Z","iopub.status.idle":"2022-07-28T06:09:17.881279Z","shell.execute_reply.started":"2022-07-28T06:09:12.515638Z","shell.execute_reply":"2022-07-28T06:09:17.880295Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"The GradientBoostingClassifier() has highest accuracy of 94.57% \n\nWe cross validate and perform higher parameter tuning to find the best parameters for GBC ","metadata":{}},{"cell_type":"markdown","source":"## Parameter-tuning","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\npipeline.set_params(clf= GradientBoostingClassifier())\npipeline.steps","metadata":{"execution":{"iopub.status.busy":"2022-07-28T06:09:17.884723Z","iopub.execute_input":"2022-07-28T06:09:17.885006Z","iopub.status.idle":"2022-07-28T06:09:17.893899Z","shell.execute_reply.started":"2022-07-28T06:09:17.884981Z","shell.execute_reply":"2022-07-28T06:09:17.892996Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"parameters = {\n    \"loss\": [\"deviance\"],\n    \"learning_rate\": [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1], \n    \"n_estimators\": [200, 350, 500, 750],\n    \"max_depth\": [3, 6, 8]\n#     \"loss\":[\"deviance\"],\n#     \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n#     \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n#     \"min_samples_leaf\": np.linspace(0.1, 0.5, 12),\n#     \"max_depth\":[3,5,8],\n#     \"max_features\":[\"log2\",\"sqrt\"],\n#     \"criterion\": [\"mae\"],\n#     \"subsample\":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],\n#     \"n_estimators\":[10]\n    }\ncvgrid = GridSearchCV(GradientBoostingClassifier(),\n                      param_grid = parameters,cv=4,scoring='accuracy')\ncvgrid.fit(xtrain,ytrain)\nprint('Train Score',cvgrid.score(xtrain,ytrain))\nprint('Test Score',cvgrid.score(xtest,ytest))\nprint('_'*25)\nprint('Best parameters',cvgrid.best_params_)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T08:29:04.049500Z","iopub.execute_input":"2022-07-28T08:29:04.049848Z","iopub.status.idle":"2022-07-28T08:41:16.993407Z","shell.execute_reply.started":"2022-07-28T08:29:04.049818Z","shell.execute_reply":"2022-07-28T08:41:16.992393Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"ypred = cvgrid.predict(xtest)\nacc = accuracy_score(ytest,ypred)\nprint('Accuracy after cv is ',acc)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T08:41:16.995509Z","iopub.execute_input":"2022-07-28T08:41:16.995868Z","iopub.status.idle":"2022-07-28T08:41:17.027844Z","shell.execute_reply.started":"2022-07-28T08:41:16.995832Z","shell.execute_reply":"2022-07-28T08:41:17.026870Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"result = pd.DataFrame({'GBC pred':ypred})\n# test_df['Y'] = result\n# result = test_df[['Unnamed: 0','Y']]\nresult.to_csv('result_gbc.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T08:45:18.301417Z","iopub.execute_input":"2022-07-28T08:45:18.302344Z","iopub.status.idle":"2022-07-28T08:45:18.314812Z","shell.execute_reply.started":"2022-07-28T08:45:18.302297Z","shell.execute_reply":"2022-07-28T08:45:18.313703Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"hyperparameters = {\n    'n_estimators'      : list(range(10, 50, 10)),\n    'max_features'      : ['auto', 'sqrt', 'log2'],\n    'criterion'         : ['gini', 'entropy'],\n    'max_depth'         : [None, 1, 2, 3, 4, 5],\n    'min_samples_split' : list(range(2,5)),\n    'min_samples_leaf'  : list(range(1,5))\n}\n\ncvgrid2 = GridSearchCV(RandomForestClassifier(random_state=1),\n                      param_grid = hyperparameters,cv=4,scoring='accuracy')\ncvgrid2.fit(xtrain,ytrain)\nprint('Train Score',cvgrid2.score(xtrain,ytrain))\nprint('Test Score',cvgrid2.score(xtest,ytest))\nprint('_'*25)\nprint('Best parameters',cvgrid2.best_params_)\npred2 = cvgrid2.predict(xtest)\nacc = accuracy_score(ytest,pred2)\nprint('Accuracy after cv is ',acc)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-28T08:17:28.296371Z","iopub.execute_input":"2022-07-28T08:17:28.296751Z","iopub.status.idle":"2022-07-28T08:24:06.522196Z","shell.execute_reply.started":"2022-07-28T08:17:28.296719Z","shell.execute_reply":"2022-07-28T08:24:06.520809Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"output = pd.DataFrame({'RF Predictions':pred2})\noutput.to_csv('RF.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-28T08:25:40.506363Z","iopub.execute_input":"2022-07-28T08:25:40.506975Z","iopub.status.idle":"2022-07-28T08:25:40.518152Z","shell.execute_reply.started":"2022-07-28T08:25:40.506939Z","shell.execute_reply":"2022-07-28T08:25:40.517213Z"},"trusted":true},"execution_count":17,"outputs":[]}]}